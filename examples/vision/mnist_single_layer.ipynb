{
 "metadata": {
  "name": "",
  "signature": "sha256:46a9695e27b63a5995945548f7cd2546a27880dcff94142b17785a345cb29fb9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import nengo\n",
      "import numpy as np\n",
      "\n",
      "from nengo_extras.data import load_mnist\n",
      "from nengo_extras.vision import Gabor, Mask\n",
      "\n",
      "\n",
      "def one_hot(labels, c=None):\n",
      "    assert labels.ndim == 1\n",
      "    n = labels.shape[0]\n",
      "    c = len(np.unique(labels)) if c is None else c\n",
      "    y = np.zeros((n, c))\n",
      "    y[np.arange(n), labels] = 1\n",
      "    return y\n",
      "\n",
      "\n",
      "rng = np.random.RandomState(9)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# --- load the data\n",
      "img_rows, img_cols = 28, 28\n",
      "\n",
      "(X_train, y_train), (X_test, y_test) = load_mnist()\n",
      "\n",
      "X_train = 2 * X_train - 1  # normalize to -1 to 1\n",
      "X_test = 2 * X_test - 1  # normalize to -1 to 1\n",
      "\n",
      "train_targets = one_hot(y_train, 10)\n",
      "test_targets = one_hot(y_test, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create the network\n",
      "\n",
      "We choose not to randomize the intercepts and max rates, so that our results are only affected by the different encoder choices.\n",
      "\n",
      "We create a function ``get_outs`` that returns the output of the network, so that we can evaluate the network statically on many images (i.e. we don't need to run the simulator)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# --- set up network parameters\n",
      "n_vis = X_train.shape[1]\n",
      "n_out = train_targets.shape[1]\n",
      "# n_hid = 300\n",
      "n_hid = 1000\n",
      "# n_hid = 3000\n",
      "\n",
      "ens_params = dict(\n",
      "    eval_points=X_train,\n",
      "    neuron_type=nengo.LIFRate(),\n",
      "    intercepts=nengo.dists.Choice([-0.5]),\n",
      "    max_rates=nengo.dists.Choice([100]),\n",
      "    )\n",
      "\n",
      "solver = nengo.solvers.LstsqL2(reg=0.01)\n",
      "# solver = nengo.solvers.LstsqL2(reg=0.0001)\n",
      "\n",
      "with nengo.Network(seed=3) as model:\n",
      "    a = nengo.Ensemble(n_hid, n_vis, **ens_params)\n",
      "    v = nengo.Node(size_in=n_out)\n",
      "    conn = nengo.Connection(\n",
      "        a, v, synapse=None,\n",
      "        eval_points=X_train, function=train_targets, solver=solver)\n",
      "    \n",
      "    \n",
      "def get_outs(sim, images):\n",
      "    _, acts = nengo.utils.ensemble.tuning_curves(a, sim, inputs=images)\n",
      "    return np.dot(acts, sim.data[conn].weights.T)\n",
      "\n",
      "def get_error(sim, images, labels):\n",
      "    return np.argmax(get_outs(sim, images), axis=1) != labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normally distributed encoders\n",
      "\n",
      "These are the standard encoders used in the NEF. Since our data is high-dimensional, they have a lot of space to cover, and do not work particularly well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "encoders = rng.normal(size=(n_hid, 28*28))\n",
      "a.encoders = encoders\n",
      "\n",
      "plt.imshow(encoders[0].reshape(28, 28), vmin=-2, vmax=2, cmap='gray')\n",
      "\n",
      "with nengo.Simulator(model) as sim:\n",
      "    test_error = 100 * get_error(sim, X_test, y_test).mean()\n",
      "    print(\"Test error: %0.2f%%\" % test_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normally distributed sparse encoders\n",
      "\n",
      "The same as before, but now each encoder has a limited receptive field. This makes each neuron only responsible for part of the image, and allows them to work together better to encode the whole image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "encoders = rng.normal(size=(n_hid, 11, 11))\n",
      "encoders = Mask((28, 28)).populate(encoders, rng=rng, flatten=True)\n",
      "a.encoders = encoders\n",
      "\n",
      "plt.imshow(encoders[0].reshape(28, 28), vmin=-2, vmax=2, cmap='gray')\n",
      "\n",
      "with nengo.Simulator(model) as sim:\n",
      "    test_error = 100 * get_error(sim, X_test, y_test).mean()\n",
      "    print(\"Test error: %0.2f%%\" % test_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gabor filter encoders\n",
      "\n",
      "Neurons in primary visual cortex have tuning that resembles Gabor filters. First, we use Gabor filters over the whole image, which does not work particularly well since a) each neuron is now responsible for the whole image again, and b) the statistics of the resulting Gabor filters do not match the statistics of the images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "encoders = Gabor().generate(n_hid, (28, 28), rng=rng).reshape(n_hid, -1)\n",
      "a.encoders = encoders\n",
      "\n",
      "plt.imshow(encoders[0].reshape(28, 28), vmin=encoders[0].min(), vmax=encoders[0].max(), cmap='gray')\n",
      "\n",
      "with nengo.Simulator(model) as sim:\n",
      "    test_error = 100 * get_error(sim, X_test, y_test).mean()\n",
      "    print(\"Test error: %0.2f%%\" % test_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sparse Gabor filter encoders\n",
      "\n",
      "Using Gabor filters that only cover part of the image results in the best performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "encoders = Gabor().generate(n_hid, (11, 11), rng=rng)\n",
      "encoders = Mask((28, 28)).populate(encoders, rng=rng, flatten=True)\n",
      "a.encoders = encoders\n",
      "\n",
      "plt.imshow(encoders[0].reshape(28, 28), vmin=encoders[0].min(), vmax=encoders[0].max(), cmap='gray')\n",
      "\n",
      "with nengo.Simulator(model) as sim:\n",
      "    test_error = 100 * get_error(sim, X_test, y_test).mean()\n",
      "    print(\"Test error: %0.2f%%\" % test_error)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}